{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_and_utils.utils import TokenizerWrap\n",
    "from models_and_utils.models import M_Model, M_Embedding, M_Novel_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    def __init__(self,model_version):\n",
    "        self.model_version=model_version\n",
    "        self.model_path=str(self.model_version).zfill(3)+'_model'\n",
    "        self.checkpoints_path=os.path.join(self.model_path,'checkpoints/')\n",
    "        self.history_path=os.path.join(self.model_path,'history.json')\n",
    "        if not os.path.exists(self.model_path):os.mkdir(self.model_path)\n",
    "        if not os.path.exists(self.checkpoints_path):os.mkdir(self.checkpoints_path)\n",
    "\n",
    "        self.encoder_type='LSTM'    #EDIT\n",
    "        self.encoder_units=256      #EDIT\n",
    "        self.decoder_type='LSTM'    #EDIT\n",
    "        self.decoder_units=2 * self.encoder_units\n",
    "        self.layer_count=2          #EDIT\n",
    "        self.beam_width=1\n",
    "        self.batch_size=10          #EDIT\n",
    "        self.attention=True\n",
    "        self.max_len_target=15\n",
    "        self.temporal_length=16\n",
    "        self.embed_path='glove.6B.100d.txt'\n",
    "        self.embed_out=100\n",
    "        self.caption_path='MSVD_captions.csv'\n",
    "        self.num_words=8000         #EDIT\n",
    "        self.seq_join_out=15\n",
    "        self.tokenizer=None\n",
    "        self.embedding=None\n",
    "        self.model=None\n",
    "        self.name=None\n",
    "        self.novel_model=False\n",
    "        \n",
    "        self.save_option()\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        if self.tokenizer == None:\n",
    "            self.tokenizer = TokenizerWrap(self)\n",
    "        return self.tokenizer\n",
    "    def get_embedding(self):\n",
    "        if self.embedding == None:\n",
    "            self.embedding = M_Embedding(self)\n",
    "        return self.embedding\n",
    "    def get_model(self):\n",
    "        if self.model == None:\n",
    "            self.model = self.new_model()\n",
    "        return self.model\n",
    "    def new_model(self):\n",
    "        if not self.novel_model: self.model = M_Model(self)\n",
    "        else: self.model = M_Novel_Model(self)\n",
    "        self.name = self.model.name\n",
    "        self.save_update()\n",
    "        return self.model\n",
    "    def save_option(self):\n",
    "        json.dump(self.__dict__,open(os.path.join(self.model_path,'info.json'),'w'))\n",
    "    def save_update(self):\n",
    "        info = json.load(open(os.path.join(self.model_path,'info.json'),'r'))\n",
    "        info.update({'name':self.name})\n",
    "        json.dump(info,open(os.path.join(self.model_path,'info.json'),'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Option(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:12, 31027.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer Created\n",
      "(1, 15, 8000)\n",
      "(10, 15, 8000)\n",
      "Model: \"en_LSTM_de_LSTM_layers_2_withAttention\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_LSTM_256 (M_Encoder) multiple                  3678223   \n",
      "_________________________________________________________________\n",
      "embedding_8000_100 (M_Embedd multiple                  1229800   \n",
      "_________________________________________________________________\n",
      "decoder_LSTM_512 (M_Decoder) multiple                  6408000   \n",
      "_________________________________________________________________\n",
      "attention (M_Attention)      multiple                  272917    \n",
      "_________________________________________________________________\n",
      "joinseq_512 (M_JoinSeq)      multiple                  512856    \n",
      "_________________________________________________________________\n",
      "stacker (Lambda)             multiple                  0         \n",
      "_________________________________________________________________\n",
      "argmax (Lambda)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 12,101,796\n",
      "Trainable params: 10,871,996\n",
      "Non-trainable params: 1,229,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = opt.get_model()\n",
    "model.build_model()\n",
    "input_list = [np.random.rand(1,opt.max_len_target,4096),np.zeros((1,1)),np.random.rand(1,opt.decoder_units),np.random.rand(1,opt.decoder_units)]\n",
    "out = model(input_list)\n",
    "print(out.shape)\n",
    "input_list = [np.random.rand(opt.batch_size,opt.max_len_target,4096),np.zeros((opt.batch_size,opt.max_len_target)),np.random.rand(opt.batch_size,opt.decoder_units),np.random.rand(opt.batch_size,opt.decoder_units)]\n",
    "out = model(input_list,training=True)\n",
    "print(out.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(opt.get_tokenizer().data_generator,(tf.float64,tf.int64,tf.int64,tf.string),((15,4096),(15),(15),()),args=[2,1000])\n",
    "train_dataset = train_dataset.shuffle(900, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = tf.data.Dataset.from_generator(opt.get_tokenizer().data_generator,(tf.float64,tf.int64,tf.int64,tf.string),((15,4096),(15),(15),()),args=[0])\n",
    "validation_dataset = validation_dataset.shuffle(900, reshuffle_each_iteration=True)\n",
    "validation_dataset = list(validation_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "             loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 100\n",
      "Checkpoint Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:07<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in epoch  100\n",
      "Epoch: 0  loss: 7.348166389465332  acc: 0.46106666666666685  bleu: {'BLEU_1': 0.0, 'BLEU_2': 0.0, 'BLEU_3': 0.0, 'BLEU_4': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:38<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in epoch  100\n",
      "Epoch: 1  loss: 5.123807325363159  acc: 0.4648666666666666  bleu: {'BLEU_1': 0.0, 'BLEU_2': 0.0, 'BLEU_3': 0.0, 'BLEU_4': 0.0}\n",
      "385.9173333644867\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "history = model.custom_fit(train_dataset, reset=True, val_data=validation_dataset, epochs=2)\n",
    "end = time()\n",
    "print(end-start)3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
