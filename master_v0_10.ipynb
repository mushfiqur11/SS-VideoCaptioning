{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, backend as K\n",
    "from tensorflow.keras.preprocessing import image \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import InceptionV3,VGG16\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Bidirectional, Dot, Concatenate, Lambda, Attention, Conv2D, Embedding, BatchNormalization, MaxPool2D, GlobalMaxPool2D, Dropout, TimeDistributed, Dense, LSTM, GRU, Flatten, RepeatVector\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from IPython.display import Video, HTML\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import deque\n",
    "import copy\n",
    "from PIL import Image\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import deque\n",
    "import copy\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm,trange\n",
    "import shutil\n",
    "import csv\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    def __init__(self,model_version):\n",
    "        self.model_version=model_version\n",
    "        self.model_path=str(self.model_version).zfill(3)+'_model'\n",
    "        self.checkpoints_path=os.path.join(self.model_path,'checkpoints/')\n",
    "        self.history_path=os.path.join(self.model_path,'history.json')\n",
    "        if not os.path.exists(self.model_path):os.mkdir(self.model_path)\n",
    "        if not os.path.exists(self.checkpoints_path):os.mkdir(self.checkpoints_path)\n",
    "\n",
    "        self.encoder_type='LSTM'    #EDIT\n",
    "        self.encoder_units=256      #EDIT\n",
    "        self.decoder_type='LSTM'    #EDIT\n",
    "        self.decoder_units=2 * self.encoder_units\n",
    "        self.layer_count=1          #EDIT\n",
    "        self.beam_width=1\n",
    "        self.batch_size=10          #EDIT\n",
    "        self.attention=True\n",
    "        self.max_len_target=15\n",
    "        self.temporal_length=16\n",
    "        self.embed_path='glove.6B.100d.txt'\n",
    "        self.embed_out=100\n",
    "        self.caption_path='MSVD_captions.csv'\n",
    "        self.num_words=8000         #EDIT\n",
    "        self.seq_join_out=15\n",
    "        self.tokenizer=None\n",
    "        self.embedding=None\n",
    "        self.model=None\n",
    "        self.name=None\n",
    "        self.novel_model=True\n",
    "        \n",
    "        self.save_option()\n",
    "        \n",
    "    def get_tokenizer(self):\n",
    "        if self.tokenizer == None:\n",
    "            self.tokenizer = TokenizerWrap(self)\n",
    "        return self.tokenizer\n",
    "    def get_embedding(self):\n",
    "        if self.embedding == None:\n",
    "            self.embedding = M_Embedding(self)\n",
    "        return self.embedding\n",
    "    def get_model(self):\n",
    "        if self.model == None:\n",
    "            self.model = self.new_model()\n",
    "        return self.model\n",
    "    def new_model(self):\n",
    "        if not self.novel_model: self.model = M_Model(self)\n",
    "        else: self.model = M_Novel_Model(self)\n",
    "        self.name = self.model.name\n",
    "        self.save_update()\n",
    "        return self.model\n",
    "    def save_option(self):\n",
    "        json.dump(self.__dict__,open(os.path.join(self.model_path,'info.json'),'w'))\n",
    "    def save_update(self):\n",
    "        info = json.load(open(os.path.join(self.model_path,'info.json'),'r'))\n",
    "        info.update({'name':self.name})\n",
    "        json.dump(info,open(os.path.join(self.model_path,'info.json'),'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Option(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    def __init__(self, options):\n",
    "        Tokenizer.__init__(self, num_words=options.num_words)\n",
    "        self.mark_start = 'ssss '\n",
    "        self.mark_end = ' eeee'\n",
    "        self.pad = ' pppp'\n",
    "        self.temporal_length = options.temporal_length\n",
    "        self.mode_dict = {0:'validation',1:'test',2:'train'}\n",
    "        \n",
    "#         self.caption_dictionary = self.get_caption_dict(options.caption_path)\n",
    "        self.caption_dictionary = self.get_full_caption_dict(options.caption_path)\n",
    "        self.texts = self.create_tokenizer(self.caption_dictionary)\n",
    "        self.fit_on_texts(self.texts)\n",
    "        \n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "        self.word_to_index = dict(zip(self.word_index.keys(), self.word_index.values()))\n",
    "    \n",
    "    def word_to_token(self, token):\n",
    "        token = 0 if word not in word_to_index else self.word_to_index[word]\n",
    "        return token\n",
    "    \n",
    "    def token_to_word(self, token):\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        text = \" \".join(words)\n",
    "        return text\n",
    "    \n",
    "    def captions_to_tokens(self, captions_list):\n",
    "        tokens = self.texts_to_sequences(captions_list)\n",
    "        tokens = pad_sequences(tokens, maxlen=self.temporal_length, padding='post', truncating='post')\n",
    "        y_in = tokens[:, 0:-1]\n",
    "        y_out = tokens[:, 1:]\n",
    "        return y_in, y_out\n",
    "        \n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def preprocess_sentence(self, w, start_end = True):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "        w = re.sub(r\"([?.!,Ã‚Â¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,Ã‚Â¿]+\", \" \", w)\n",
    "        w = w.strip()\n",
    "        if start_end:\n",
    "            w = self.mark_start + w + self.mark_end\n",
    "            for i in range(12):\n",
    "                w = w+self.pad\n",
    "        return w\n",
    "\n",
    "    def mark_captions(self, captions_list):\n",
    "        captions_marked = [self.preprocess_sentence(caption)\n",
    "                            for caption in captions_list]\n",
    "        return captions_marked\n",
    "    \n",
    "    def get_full_caption_dict(self, path):\n",
    "        df = pd.read_csv(path,encoding='utf-8')\n",
    "        df = df.iloc[:,1::]\n",
    "        df = df.values.tolist()\n",
    "        caption_dictionary = {}\n",
    "        for data in df:\n",
    "            count = 0\n",
    "            for caption in data[1:]:\n",
    "                if str(caption)=='nan':\n",
    "                    break\n",
    "                else:\n",
    "                    count+=1\n",
    "            caption_dictionary.update({data[0]:self.mark_captions(data[1:count])})\n",
    "        return caption_dictionary\n",
    "    \n",
    "    def get_caption_dict(self,path):\n",
    "        captions = pd.read_csv(path)\n",
    "        parents = captions['FileName']\n",
    "        caption_list = captions[['0','1','2','3','4']].values\n",
    "        caption_dict = dict()\n",
    "        for i in range(len(parents)):\n",
    "            caption_dict.update({parents[i]:self.mark_captions(caption_list[i])})\n",
    "        return caption_dict\n",
    "    \n",
    "    def create_tokenizer(self,caption_dictionary):\n",
    "        cap_list = []\n",
    "        for parent,rows in caption_dictionary.items():\n",
    "            for row in rows:\n",
    "                cap_list.append(row)\n",
    "        return cap_list\n",
    "    def clean_cap(self, captions):\n",
    "        clean_captions = []\n",
    "        for caption in captions:\n",
    "            caption = caption.split(' ')\n",
    "            clean_caption = []\n",
    "            for word in caption:\n",
    "                if word not in ['eeee','pppp','ssss','.']:\n",
    "                    clean_caption.append(word)\n",
    "            clean_captions.append(clean_caption)\n",
    "        return clean_captions\n",
    "\n",
    "    def get_data_list(self,path):\n",
    "        return list(pd.read_csv(path)['0'])\n",
    "\n",
    "    def get_parent(self,path):\n",
    "        return '_'.join(path.split('_')[1:4])\n",
    "\n",
    "    def data_generator(self,mode=0, data_size=1000000):\n",
    "        assert mode in [0,1,2],\"Invalid mode\"\n",
    "        mode = self.mode_dict[mode]\n",
    "        data_dir = 'data_pickle'\n",
    "\n",
    "        data_path = mode+'.csv'\n",
    "        data_list = get_data_list(os.path.join(data_dir,data_path))\n",
    "        data_len = min(data_size,len(data_list))\n",
    "    #     print(\"Working with \",data_len,\" data items out of \",len(data_list))\n",
    "        caption_dict = self.caption_dictionary\n",
    "\n",
    "        for data in data_list[:data_len]:\n",
    "            parent = get_parent(data)\n",
    "            y = random.choice(caption_dict[parent])\n",
    "            y_in, y_out = self.captions_to_tokens([y])\n",
    "            with open(os.path.join(data_dir,mode,data),'rb') as f:\n",
    "                X = pickle.load(f)\n",
    "            yield tf.convert_to_tensor(X,dtype=tf.float64), tf.convert_to_tensor(y_in[0],dtype=tf.int64), tf.convert_to_tensor(y_out[0],dtype=tf.int64), tf.convert_to_tensor(parent,dtype=tf.string)\n",
    "\n",
    "tokenizer = opt.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class M_Encoder(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = '_'.join(['encoder',option.encoder_type,str(option.encoder_units)])\n",
    "        super(M_Encoder, self).__init__(name=name)\n",
    "        self.option=option\n",
    "        assert option.encoder_units>0 and type(option.encoder_units)==int,\"Encoder type must be positive integer\"\n",
    "        \n",
    "        self.timeDistDense = TimeDistributed(Dense(option.encoder_units, activation = 'relu',name='dense'), name='time_distributed_1')\n",
    "        \n",
    "        self.encoder = []\n",
    "        for i in range(option.layer_count):\n",
    "            if option.encoder_type=='GRU':\n",
    "                self.encoder.append(Bidirectional(GRU(option.encoder_units, return_state=False, return_sequences = True, dropout = .5, name='en_gru'+str(i).zfill(2)),\n",
    "                                        name='encoder'+str(i).zfill(2)))\n",
    "            elif option.encoder_type=='LSTM':\n",
    "                self.encoder.append(Bidirectional(LSTM(option.encoder_units, return_state=False, return_sequences = True, dropout = .5, name='en_lstm'+str(i).zfill(2)),\n",
    "                                         name='encoder'+str(i).zfill(2)))\n",
    "            else:\n",
    "                assert False,\"invalid encoder type\"\n",
    "            \n",
    "        if option.seq_join_out>0:\n",
    "            self.timeDistDense2 = TimeDistributed(Dense(option.seq_join_out, activation = 'relu',name='dense'), name='time_distributed_2')            \n",
    "            \n",
    "        self.build((None, option.max_len_target, 4096))\n",
    "    \n",
    "    def join_seq(self, x):\n",
    "        x = tf.reshape(x, (-1,self.option.seq_join_out*self.option.max_len_target))\n",
    "        return x\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x_time = self.timeDistDense(inputs)\n",
    "        x_en = x_time\n",
    "        x_en_list = []\n",
    "        for i in range(self.option.layer_count):\n",
    "            x_en = self.encoder[i](x_en)\n",
    "            x_en_list.append(x_en)\n",
    "        if self.option.seq_join_out>0:\n",
    "            x_time = self.timeDistDense2(x_time)\n",
    "            x_time = self.join_seq(x_time)\n",
    "            x_time = tf.expand_dims(x_time,axis=-2)\n",
    "        return x_en, x_time, x_en_list\n",
    "\n",
    "class M_JoinSeq(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = '_'.join(['joinseq',str(option.decoder_units)])\n",
    "        super(M_JoinSeq, self).__init__(name=name)\n",
    "        self.join_concat = Concatenate(axis=2, name='join_concat')\n",
    "        self.join_dense = Dense(2*option.encoder_units+option.embed_out, activation='relu', name='join_dense')\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x = self.join_concat(inputs)\n",
    "        x = self.join_dense(x)\n",
    "        return x\n",
    "    \n",
    "class M_Embedding(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = '_'.join(['embedding',str(option.num_words),str(option.embed_out)])\n",
    "        super(M_Embedding, self).__init__(name=name)\n",
    "        self.word2idx = option.get_tokenizer().word_index\n",
    "        embeddings_index = self.embeddings_index_creator(option.embed_path)\n",
    "        self.embed_in = len(self.word2idx) + 1\n",
    "        self.embedding_matrix = self.embedding_matrix_creator(embeddings_index, word_index=self.word2idx,embedding_out=option.embed_out)\n",
    "        self.embedding = Embedding(self.embed_in, option.embed_out, name='embedding', trainable = False)\n",
    "        self.build((None,))\n",
    "        self.set_weights([self.embedding_matrix])\n",
    "        print('Embedding Layer Created')\n",
    "        \n",
    "    def embeddings_index_creator(self, embed_path):\n",
    "        embeddings_index = {}\n",
    "        with open(embed_path, encoding='utf-8') as f:\n",
    "            for line in tqdm(f,file=None):\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                try:\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "                except:\n",
    "                    pass\n",
    "            f.close()\n",
    "        return embeddings_index\n",
    "    \n",
    "    def embedding_matrix_creator(self, embeddings_index, word_index, embedding_out):\n",
    "        embedding_matrix = np.zeros((len(word_index) + 1, embedding_out))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        return x\n",
    "\n",
    "class M_Decoder(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = '_'.join(['decoder',option.decoder_type,str(option.decoder_units)])\n",
    "        super(M_Decoder, self).__init__(name=name)\n",
    "        \n",
    "        assert option.decoder_units>0 and type(option.decoder_units)==int,\"Encoder type must be positive integer\"\n",
    "        \n",
    "        if option.decoder_type=='GRU':\n",
    "            self.decoder = GRU(option.decoder_units, return_state = True, return_sequences = True, dropout = .5,name='decoder')\n",
    "        elif option.decoder_type=='LSTM':\n",
    "            self.decoder = LSTM(option.decoder_units, return_state = True, return_sequences = True, dropout = .5,name='decoder')\n",
    "        else:\n",
    "            assert False,\"invalid encoder type\"\n",
    "        \n",
    "        self.decoder_dense = Dense(option.num_words, activation = 'sigmoid', name='decoder_dense')\n",
    "        \n",
    "        self.build((None, 1, 2*option.encoder_units+option.embed_out))\n",
    "    \n",
    "    def call(self,inputs,initial_state=None):\n",
    "        x = self.decoder(inputs,initial_state=initial_state)\n",
    "        if type(x)==list:\n",
    "            out = self.decoder_dense(x[0])\n",
    "            return out, x[1:]\n",
    "        else:\n",
    "            out = self.decoder_dense(x)\n",
    "            return x\n",
    "    \n",
    "class M_Attention(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = '_'.join(['attention'])\n",
    "        super(M_Attention,self).__init__(name=name)\n",
    "        self.attn_dense0 = Dense(2*option.encoder_units,activation='tanh',name='attn_dense0')\n",
    "        self.attn_repeat_layer = RepeatVector(option.max_len_target,name='repeat_vector')\n",
    "        self.attn_concat_layer = Concatenate(axis=-1,name='attn_concat')\n",
    "        self.attn_dense1 = Dense(10, activation='tanh', name='attn_dense1')\n",
    "        self.attn_dense2 = Dense(1, activation=self.softmax_over_time, name='attn_dense2')\n",
    "        self.attn_dot = Dot(axes=1,name='attn_dot')\n",
    "        \n",
    "        self.context_last_word_concat_layer = Concatenate(axis=2,name='concat_last_word')\n",
    "    \n",
    "        self.build([(None,option.max_len_target,2*option.encoder_units),(None,1,option.embed_out),(1,option.decoder_units,)])\n",
    "\n",
    "    def softmax_over_time(self,x):\n",
    "        assert (K.ndim(x)>2),\"x dims too small\"\n",
    "        e = K.exp(x - K.max(x,axis = 1, keepdims = True))\n",
    "        s = K.sum(e, axis = 1, keepdims = True)\n",
    "        return e/s\n",
    "\n",
    "    def one_step_attention(self, h,st_1):\n",
    "        st_1 = self.attn_repeat_layer(st_1)\n",
    "        x = self.attn_concat_layer([h,st_1])\n",
    "        x = self.attn_dense1(x)\n",
    "        x = self.attn_dense2(x)\n",
    "        context = self.attn_dot([x,h])\n",
    "        return context\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        en_output, xt, s = inputs\n",
    "        s = self.attn_dense0(s)\n",
    "        context = self.one_step_attention(en_output,s)\n",
    "        print(context)\n",
    "        decoder_input = self.context_last_word_concat_layer([context,xt])\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = M_Encoder(opt)\n",
    "# out = encoder(np.random.rand(10,15,4096))\n",
    "# print(out[0].shape)\n",
    "# # encoder.summary()\n",
    "\n",
    "# attention = M_Attention(opt)\n",
    "# out = attention([np.random.rand(10,15,512),np.random.rand(10,1,100),np.random.rand(10,512)])\n",
    "# print(out.shape)\n",
    "\n",
    "decoder = M_Decoder(opt)\n",
    "# decoder.summary()\n",
    "out = decoder.call(np.random.rand(10,1,612),initial_state=[])\n",
    "# print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.recurrent.LSTMCell at 0x7f4d8d1626a0>,\n",
       " ListWrapper([])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.layers[0]._layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Model(tf.keras.Model):\n",
    "    def __init__(self,option):\n",
    "        name = self.get_model_name(option)\n",
    "        super(M_Model, self).__init__(name=name)\n",
    "        assert option.layer_count > 0 and type(option.layer_count)==int,'Layer_count must be positive'\n",
    "        assert option.attention==True,'Code for no attention is not yet available'\n",
    "\n",
    "        self.option = option\n",
    "    \n",
    "        self.word2idx = option.get_tokenizer().word_index\n",
    "        self.idx2word = { v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "        self.eos = self.word2idx['eeee']\n",
    "\n",
    "        self.encoder = M_Encoder(option)\n",
    "\n",
    "        self.embedding = option.get_embedding()\n",
    "\n",
    "        self.decoder = M_Decoder(option)\n",
    "\n",
    "        self.attention = [M_Attention(option)] if option.attention else None\n",
    "        \n",
    "        self.join_seq = M_JoinSeq(option) if option.seq_join_out else None \n",
    "\n",
    "        self.stacker = Lambda(self.stack_and_transpose,name='stacker')\n",
    "\n",
    "        self.argmax = Lambda(self.arg_max_func, name='argmax')\n",
    "        self.flatten = Flatten(name='flatten')\n",
    "        \n",
    "        self.hist = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        build_input_shape = []\n",
    "        build_input_shape.append((None, self.option.max_len_target,4096))\n",
    "        build_input_shape.append((None, 1))\n",
    "        if self.attention:\n",
    "            build_input_shape.append((None, self.option.decoder_units))\n",
    "            build_input_shape.append((None, self.option.decoder_units))\n",
    "        self.build(input_shape=build_input_shape)\n",
    "    \n",
    "    def arg_max_func(self, x):\n",
    "        x = tf.math.argmax(x,axis=-1)\n",
    "        x = tf.expand_dims(x,-1)\n",
    "        # x = tf.expand_dims(x,-1)\n",
    "        return x\n",
    "\n",
    "    def stack_and_transpose(self,x):\n",
    "        x = K.stack(x)\n",
    "        x = K.permute_dimensions(x, pattern=(1,0,3,2))\n",
    "        x = tf.squeeze(x,axis=-1)\n",
    "        return x\n",
    "\n",
    "    def get_model_name(self,option):\n",
    "        model_name='_'.join(['en',option.encoder_type,'de',option.decoder_type,'layers',str(option.layer_count)])\n",
    "        if option.attention:\n",
    "            model_name=model_name+'_withAttention'\n",
    "        return model_name\n",
    "    \n",
    "    def decode_sequence(self, dataset, start=0, length=1, log=False, return_parents=False, save=True):\n",
    "        out_paragraph = []\n",
    "        BLEU_1 = 0\n",
    "        BLEU_2 = 0\n",
    "        BLEU_3 = 0\n",
    "        BLEU_4 = 0\n",
    "        original = []\n",
    "        parents = []\n",
    "        for i in range(0, length):\n",
    "            input_seq,y_in,y_out,parent = dataset[start+i]\n",
    "            parent = parent.decode()\n",
    "            parents.append(parent)\n",
    "            input_data = []\n",
    "            input_data.append(tf.expand_dims(input_seq,axis = 0))\n",
    "            target_seq = np.zeros((1,1))\n",
    "            target_seq[0,0] = self.word2idx['ssss']\n",
    "            input_data.append(target_seq)\n",
    "            if self.attention:\n",
    "                s = np.zeros((1,self.option.decoder_units))\n",
    "                input_data.append(s)\n",
    "                c = np.zeros((1,self.option.decoder_units))\n",
    "                input_data.append(c)\n",
    "\n",
    "            outputs = self.predict(input_data)\n",
    "            output_seq = []\n",
    "            for out in outputs[0]:\n",
    "                idx = self.argmax(out).numpy()[0]\n",
    "                if self.eos == idx or self.word2idx['pppp']==idx:\n",
    "                    break\n",
    "                word = ' '\n",
    "                if idx>0:\n",
    "                    word = self.idx2word[idx]\n",
    "                    output_seq.append(word)\n",
    "            sentence = ' '.join(output_seq)\n",
    "            out_paragraph.append(sentence)\n",
    "            references = self.option.get_tokenizer().clean_cap(self.option.get_tokenizer().caption_dictionary[parent])\n",
    "            BLEU_1+=corpus_bleu([references], [output_seq], weights=(1  ,  0,  0,  0))*100.00\n",
    "            BLEU_2+=corpus_bleu([references], [output_seq], weights=(1/2,1/2,  0,  0))*100.00\n",
    "            BLEU_3+=corpus_bleu([references], [output_seq], weights=(1/3,1/3,1/3,  0))*100.00\n",
    "            BLEU_4+=corpus_bleu([references], [output_seq], weights=(1/4,1/4,1/4,1/4))*100.00\n",
    "            if log:\n",
    "                print([references], [output_seq])\n",
    "            sentence = ' '.join(references[0])\n",
    "            original.append(sentence)\n",
    "        scores = {'BLEU_1':BLEU_1/max(length,1),'BLEU_2':BLEU_2/max(length,1),'BLEU_3':BLEU_3/max(length,1),'BLEU_4':BLEU_4/max(length,1)}\n",
    "        if save:\n",
    "            decoded_dict = {}\n",
    "            for i in range(len(out_paragraph)):\n",
    "                decoded_dict.update({i:{'pred':out_paragraph[i],'real':original[i],'parent':parents[i]}})\n",
    "            try:\n",
    "                json.dump(decoded_dict,open(os.path.join(self.option.model_path, str(self.hist.epoch[-1]+1).zfill(2)+'_sample.json'),'w'))\n",
    "            except:\n",
    "                json.dump(decoded_dict,open(os.path.join(self.option.model_path, '00_sample.json'),'w'))\n",
    "        if return_parents:\n",
    "            return out_paragraph, scores, original, parents\n",
    "        return out_paragraph, scores, original\n",
    "    \n",
    "    def custom_fit(self, dataset, val_data=None, epochs=1, reset=False):\n",
    "        save_path=self.option.checkpoints_path\n",
    "        if epochs:\n",
    "            total_batches = sum(1 for _ in dataset.padded_batch(self.option.batch_size).as_numpy_iterator())\n",
    "            print('Total Batches:',total_batches)\n",
    "        if os.path.exists(self.option.history_path) and not reset:\n",
    "            self.hist = tf.keras.callbacks.History()\n",
    "            self.load_weights(save_path)\n",
    "            self.hist.set_model(self)\n",
    "            self.hist.on_train_begin()\n",
    "            self.hist.history = json.load(open(self.option.history_path, 'r'))\n",
    "            self.hist.epoch = self.hist.history['epoch']\n",
    "            curr_epoch = self.hist.epoch[-1]+1\n",
    "            print(\"Starting training from \",curr_epoch,\" epochs\")\n",
    "        else:\n",
    "            self.hist = tf.keras.callbacks.History()\n",
    "            self.hist.set_model(self)\n",
    "            self.hist.on_train_begin()\n",
    "            curr_epoch = 0\n",
    "            self.save_weights(save_path)\n",
    "            print(\"Checkpoint Initialized\")\n",
    "        for epoch in range(curr_epoch,epochs+curr_epoch):\n",
    "            score=0\n",
    "            loss = 0\n",
    "            accuracy = 0\n",
    "            batch_count = 0\n",
    "            data = dataset.shuffle(900, reshuffle_each_iteration=True)\n",
    "            data = data.padded_batch(self.option.batch_size).as_numpy_iterator()\n",
    "            for i in trange(total_batches):\n",
    "                element = next(data)\n",
    "                X, y_in, y_out,parent = element\n",
    "                BATCH_SIZE = X.shape[0]\n",
    "                input_list = []\n",
    "                input_list.append(X)\n",
    "                input_list.append(y_in)\n",
    "                if self.option.attention:\n",
    "                    z1 = np.zeros((BATCH_SIZE,self.option.decoder_units))\n",
    "                    input_list.append(z1)\n",
    "                    z2 = np.zeros((BATCH_SIZE,self.option.decoder_units))\n",
    "                    input_list.append(z2)\n",
    "                loss_t, accuracy_t = self.train_on_batch(\n",
    "                    input_list, \n",
    "                    tf.keras.utils.to_categorical(y_out, num_classes = self.option.num_words)\n",
    "                )\n",
    "                if not np.any(np.isnan(loss_t)): \n",
    "                    loss+=loss_t\n",
    "                    accuracy+=accuracy_t\n",
    "                    batch_count+=1\n",
    "            if not val_data==None:\n",
    "                _,score,_ = self.decode_sequence(val_data,start=0,length=5+epoch*10)\n",
    "            if batch_count:\n",
    "                loss=loss/batch_count\n",
    "                accuracy=accuracy/batch_count\n",
    "                print(\"Batches in epoch \", batch_count)\n",
    "                self.hist.on_epoch_end(epoch=epoch,logs={'loss':loss,'accuracy':accuracy,'bleu':score,'epoch':epoch})\n",
    "                print('Epoch:',epoch,' loss:',loss,' acc:',accuracy,' bleu:',score)\n",
    "                self.save_weights(save_path)\n",
    "                json.dump(self.hist.history, open(self.option.history_path, 'w'))\n",
    "        return self.hist\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_inputs, decoder_inputs, init_s, init_c = inputs\n",
    "        state = [init_s]\n",
    "        if self.option.decoder_type=='LSTM': state.append(init_c)\n",
    "\n",
    "        en_output, en_time, en_output_list = self.encoder(encoder_inputs)\n",
    "        outputs = []\n",
    "        \n",
    "        if not training: target_seq = decoder_inputs\n",
    "\n",
    "        for t in range(self.option.max_len_target):\n",
    "            if training:\n",
    "                selector = Lambda(lambda x,t: x[:, t:t+1],arguments={'t':t},name='lambda'+str(t))\n",
    "                target_seq = selector(decoder_inputs,t)   \n",
    "\n",
    "            xt = self.embedding(target_seq)\n",
    "            decoder_input = self.attention[0]([en_output,xt,state[0]])\n",
    "            if self.join_seq: decoder_input = self.join_seq([decoder_input,en_time])\n",
    "            out, state = self.decoder(decoder_input, initial_state=state)\n",
    "            outputs.append(out)\n",
    "\n",
    "            if not training:\n",
    "                flat = self.flatten(out)\n",
    "                idx = self.argmax(flat)\n",
    "                target_seq = idx\n",
    "        outputs = self.stacker(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M_Novel_Model(M_Model):\n",
    "    def __init__(self,option):\n",
    "        super(M_Novel_Model, self).__init__(option=option)\n",
    "        self.attention = []\n",
    "        for layer in range(option.layer_count):\n",
    "            self.attention.append(M_Attention(option))\n",
    "        if option.layer_count>1: \n",
    "            self.enc_stack = Concatenate(-1,name='enc_stacker')\n",
    "            self.stacked_dense = Dense(2*self.option.encoder_units, activation='relu', name='stacked_dense')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_inputs, decoder_inputs, init_s, init_c = inputs\n",
    "        state = [init_s]\n",
    "        if self.option.decoder_type=='LSTM': state.append(init_c)\n",
    "\n",
    "        en_output, en_time, en_output_list = self.encoder(encoder_inputs)\n",
    "        outputs = []\n",
    "        \n",
    "        if not training: target_seq = decoder_inputs\n",
    "\n",
    "        for t in range(self.option.max_len_target):\n",
    "            if training:\n",
    "                selector = Lambda(lambda x,t: x[:, t:t+1],arguments={'t':t},name='lambda'+str(t))\n",
    "                target_seq = selector(decoder_inputs,t)   \n",
    "\n",
    "            xt = self.embedding(target_seq)\n",
    "            encoder_out = []\n",
    "            for layer in range(len(en_output_list)):\n",
    "                encoder_out.append(self.attention[layer]([en_output_list[layer],xt,state[0]]))\n",
    "            if len(encoder_out)>1: \n",
    "                decoder_input = self.enc_stack(encoder_out)\n",
    "                decoder_input = self.stacked_dense(decoder_input)\n",
    "                \n",
    "            else: decoder_input = encoder_out[0]\n",
    "            \n",
    "            if self.join_seq: decoder_input = self.join_seq([decoder_input,en_time])\n",
    "            out, state = self.decoder(decoder_input, initial_state=state)\n",
    "            outputs.append(out)\n",
    "\n",
    "            if not training:\n",
    "                flat = self.flatten(out)\n",
    "                idx = self.argmax(flat)\n",
    "                target_seq = idx\n",
    "        outputs = self.stacker(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:14, 28372.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer Created\n",
      "Tensor(\"attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_1/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_2/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_3/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_4/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_5/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_6/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_7/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_8/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_9/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_10/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_11/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_12/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_13/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Tensor(\"attention_14/attn_dot/MatMul:0\", shape=(None, 1, 512), dtype=float64)\n",
      "Model: \"en_LSTM_de_LSTM_layers_1_withAttention\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_LSTM_256 (M_Encoder) multiple                  2103311   \n",
      "_________________________________________________________________\n",
      "embedding_8000_100 (M_Embedd multiple                  1229800   \n",
      "_________________________________________________________________\n",
      "decoder_LSTM_512 (M_Decoder) multiple                  6408000   \n",
      "_________________________________________________________________\n",
      "joinseq_512 (M_JoinSeq)      multiple                  512856    \n",
      "_________________________________________________________________\n",
      "stacker (Lambda)             multiple                  0         \n",
      "_________________________________________________________________\n",
      "argmax (Lambda)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "attention (M_Attention)      multiple                  272917    \n",
      "=================================================================\n",
      "Total params: 10,526,884\n",
      "Trainable params: 9,297,084\n",
      "Non-trainable params: 1,229,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = M_Novel_Model(opt)\n",
    "model.build_model()\n",
    "# input_list = [np.random.rand(1,opt.max_len_target,4096),np.zeros((1,1)),np.random.rand(1,opt.decoder_units),np.random.rand(1,opt.decoder_units)]\n",
    "# out = model(input_list)\n",
    "# print(out.shape)\n",
    "# input_list = [np.random.rand(opt.batch_size,opt.max_len_target,4096),np.zeros((opt.batch_size,opt.max_len_target)),np.random.rand(opt.batch_size,opt.decoder_units),np.random.rand(opt.batch_size,opt.decoder_units)]\n",
    "# out = model(input_list,training=True)\n",
    "# print(out.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_generator(opt.get_tokenizer().data_generator,(tf.float64,tf.int64,tf.int64,tf.string),((15,4096),(15),(15),()),args=[2,50])\n",
    "train_dataset = train_dataset.shuffle(900, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = list(train_dataset.padded_batch(opt.batch_size).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "NameError: name 'get_data_list' is not defined\nTraceback (most recent call last):\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 785, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-5-17baf499fe40>\", line 114, in data_generator\n    data_list = get_data_list(os.path.join(data_dir,data_path))\n\nNameError: name 'get_data_list' is not defined\n\n\n\t [[{{node PyFunc}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: NameError: name 'get_data_list' is not defined\nTraceback (most recent call last):\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 785, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-5-17baf499fe40>\", line 114, in data_generator\n    data_list = get_data_list(os.path.join(data_dir,data_path))\n\nNameError: name 'get_data_list' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9a279559f36e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3724\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3725\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3722\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3724\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: NameError: name 'get_data_list' is not defined\nTraceback (most recent call last):\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/mushfiqur11/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 785, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-5-17baf499fe40>\", line 114, in data_generator\n    data_list = get_data_list(os.path.join(data_dir,data_path))\n\nNameError: name 'get_data_list' is not defined\n\n\n\t [[{{node PyFunc}}]]"
     ]
    }
   ],
   "source": [
    "validation_dataset = tf.data.Dataset.from_generator(opt.get_tokenizer().data_generator,(tf.float64,tf.int64,tf.int64,tf.string),((15,4096),(15),(15),()),args=[0])\n",
    "validation_dataset = validation_dataset.shuffle(900, reshuffle_each_iteration=True)\n",
    "validation_dataset = list(validation_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "             loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 5\n",
      "Checkpoint Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in epoch  5\n",
      "Epoch: 0  loss: 8.634870338439942  acc: 0.2533333333333333  bleu: {'BLEU_1': 0.0, 'BLEU_2': 0.0, 'BLEU_3': 0.0, 'BLEU_4': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in epoch  5\n",
      "Epoch: 1  loss: 8.430844497680663  acc: 0.47733333333333333  bleu: {'BLEU_1': 0.0, 'BLEU_2': 0.0, 'BLEU_3': 0.0, 'BLEU_4': 0.0}\n"
     ]
    }
   ],
   "source": [
    "history = model.custom_fit(train_dataset, reset=True, val_data=validation_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "decoded_outputs = model.decode_sequence(validation_dataset,length=20,return_parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = decoded_outputs\n",
    "decoded_dict = {}\n",
    "for i in range(len(a)):\n",
    "    decoded_dict.update({i:{'pred':a[i],'real':c[i],'parent':d[i]}})\n",
    "json.dump(decoded_dict,open('output_sample.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TokenizerWrap.data_generator of <__main__.TokenizerWrap object at 0x7eff56263690>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.get_tokenizer().data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.data_generator(mode=0, data_size=1000000)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hist.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_LSTM_de_LSTM_layers_2_withAttention'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = opt.get_tokenizer().caption_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = opt.get_tokenizer().clean_cap(ref[\"3opDcpPxllE_50_66\"])\n",
    "output_seq = [\"a man is driving a car\".split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 100.0\n",
      "b2 100.0\n",
      "b3 100.0\n",
      "b4 90.36020036098448\n"
     ]
    }
   ],
   "source": [
    "print('b1',corpus_bleu([references], output_seq, weights=(1,0,0,0))*100.00)\n",
    "print('b2',corpus_bleu([references], output_seq, weights=(1/2,1/2,0,0))*100.00)\n",
    "print('b3',corpus_bleu([references], output_seq, weights=(1/3,1/3,1/3,0))*100.00)\n",
    "print('b4',corpus_bleu([references], output_seq, weights=(1/4,1/4,1/4,1/4))*100.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'powdered', 'substance', 'is', 'being', 'shifted', 'into', 'a', 'pan']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
